import gradio as gr
import json
from perception import Perception
from action import  Action
from brain import Brain
from lucy_agent import LucyAgent

def save_to_file(file_path:str, conversations)-> None:
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(conversations, f, ensure_ascii=False, indent=4)

def load_prompts(json_file: str) -> dict:
    with open(json_file, 'r',encoding='utf-8') as f:
        data = json.load(f)
    return {item['key']: item['prompt'] for item in data}

def save_agent_json(agent_brain):
    # å­˜åˆ°â€œä¸­æ–‡name.jsonâ€å½“ä¸­ï¼Œä¸å½±å“ç”¨æ¥åˆå§‹åŒ–çš„æ–‡ä»¶
    name = agent_brain.name
    file_path = f"../resource/{name}.json"
    # ç”¨æ¥ä¿®æ”¹åˆå§‹åŒ–æ–‡ä»¶
    # file_path = f"../resource/hutao.json"
    try:
        with open(file_path, "w", encoding="utf-8") as json_file:
            json.dump(agent_brain.to_json(), json_file, indent=4, ensure_ascii=False)
    except IOError:
        print("æ— æ³•å†™å…¥æ–‡ä»¶ã€‚")

if __name__ == "__main__":
    # ç”¨åˆå§‹jsonè¿›è¡Œåˆå§‹åŒ–
    with open("../resource/èƒ¡æ¡ƒ.json", "r", encoding="utf-8") as json_file:
        loaded_data = json.load(json_file)

    perception = Perception()
    action = Action()
    hutao = LucyAgent(perception=perception, brain=Brain.from_json(loaded_data), action=action)

    # åˆ›å»ºä¸€ä¸ª Gradio ç•Œé¢
    with gr.Blocks() as demo:
        # åˆ›å»ºä¸€ä¸ªçŠ¶æ€å¯¹è±¡ï¼Œç”¨äºå­˜å‚¨å†å²è®°å½•
        state = gr.State([])


        with gr.Tab("å’Œèƒ¡æ¡ƒå¯¹è¯ï¼\U0001F917"):
            def talk_with_hutao(input, history=None):
                if history is None:
                    history = []
                response, history, thought = hutao.brain.cot_chat(input, history)

                input = f"æ”¶åˆ°äº†æ¥è‡ªhadiçš„è¯¢é—®ï¼š{input}"
                output = f"è¿›è¡Œäº†æ€è€ƒï¼š{thought},åšå‡ºäº†å›å¤ï¼š{response}"
                memory = hutao.brain.create_memory(input,output)
                hutao.brain.add_memory(memory)

                history_text = ""  # åˆå§‹åŒ–å†å²è®°å½•æ–‡æœ¬
                for chat in history:  # éå†å†å²è®°å½•
                    history_text = history_text + chat + "\n"
                save_to_file("../resource/conversations.json", history)

                # è·å–å¿ƒæƒ…é©±åŠ¨çš„è¡¨æƒ…åŒ…
                hutao.brain.fsm.mood_transition(input, thought)
                image_path = hutao.brain.fsm.get_current_emoji()

                # ç”Ÿæˆå“åº”çš„éŸ³é¢‘
                default_audio_path = "../resource/audios/è¿™æ˜¯ä¸€æ®µæµ‹è¯•éŸ³é¢‘å“Ÿ.wav"
                # audio_file_path = apis.genshin_tts(text=response.lstrip("èƒ¡æ¡ƒ:"), speaker="èƒ¡æ¡ƒ")
                audio_file_path = default_audio_path

                save_agent_json(hutao.brain)

                return history_text, audio_file_path, image_path  # è¿”å›å†å²è®°å½•æ–‡æœ¬å’ŒéŸ³é¢‘æ–‡ä»¶è·¯å¾„

            with gr.Row():
                with gr.Column():
                    # åˆ›å»ºä¸€ä¸ªç”¨äºæ˜¾ç¤ºå†å²è®°å½•çš„æ–‡æœ¬æ¡†
                    history_box = gr.Textbox(lines=10, label="å¯¹è¯å†å²è®°å½•\U0001F4DD")
                    # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ï¼Œç”¨äºè¾“å…¥æ–‡æœ¬
                    txt = gr.Textbox(show_label=False, placeholder="è¾“å…¥æ–‡æœ¬ï¼Œä¾‹å¦‚\"èƒ¡æ¡ƒå¯ä»¥ç»™æˆ‘æ¥ä¸€æ¯å’–å•¡å—ï¼Ÿ\"\U0001F4AC")

                with gr.Column():
                    # åˆ›å»ºä¸€ä¸ªéŸ³é¢‘æ’­æ”¾å™¨
                    audio_box = gr.Audio(label="èƒ¡æ¡ƒè¿”å›çš„éŸ³é¢‘\U0001F3B5")
                    # åˆ›å»º Image ç»„ä»¶å¹¶è®¾ç½®é»˜è®¤å›¾ç‰‡
                    image_box = gr.Image(label="èƒ¡æ¡ƒå¿ƒæƒ…å¯¹åº”çš„è¡¨æƒ…\U0001F60A",height = 200)

            # æŒ‰é’®è¢«ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ talk_with_hutao å‡½æ•°ï¼Œå¹¶å°†æ–‡æœ¬æ¡†çš„å†…å®¹å’ŒçŠ¶æ€å¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œå°†å†å²è®°å½•æ–‡æœ¬æ¡†å’ŒéŸ³é¢‘æ’­æ”¾å™¨ä½œä¸ºè¾“å‡º
            button = gr.Button("å‘é€ \U0001F600")
            button.click(talk_with_hutao, [txt, state], [history_box, audio_box, image_box])

        with gr.Tab("è§‚å¯Ÿå’Œç®¡ç†èƒ¡æ¡ƒçš„Brainæ¨¡å—çŠ¶æ€ğŸ”§"):
            with gr.Row():
                with gr.Column():
                    agent_state = gr.Textbox(lines=25, max_lines=25, label="èƒ¡æ¡ƒçš„Brainæ¨¡å—çŠ¶æ€\U0001F4C4")
                    button = gr.Button("æŸ¥è¯¢ \U0001F600")
                    button.click(hutao.brain.show_info ,inputs=[], outputs=agent_state)

                with gr.Column():
                    def del_memory(memory_index):
                        memory_str = hutao.brain.del_memory(mode="single", index=memory_index)
                        save_agent_json(hutao.brain)
                        return memory_str

                    memory_keys = list(range(len(hutao.brain.memory_stream)))
                    gr.Interface(fn=del_memory,
                                 # title="åˆ é™¤è®°å¿†ğŸ§ ",
                                 inputs=gr.Dropdown(memory_keys, label="é€‰æ‹©è¦åˆ é™¤çš„è®°å¿†åºå·\U0001F600"),
                                 outputs=gr.Textbox(label="å·²åˆ é™¤çš„èƒ¡æ¡ƒè®°å¿†ğŸ§ "),
                                 allow_flagging="never")

                    def del_knowledge(knowledge_index):
                        knowledge_index = int(knowledge_index)
                        knowledge_str = hutao.brain.del_knowledge(mode="single",index=knowledge_index)
                        save_agent_json(hutao.brain)
                        return knowledge_str

                    knowledge_keys = list(range(len(hutao.brain.basic_knowledge)))
                    gr.Interface(fn=del_knowledge,
                                 # title="åˆ é™¤çŸ¥è¯†ğŸ“š",
                                 inputs=gr.Dropdown(knowledge_keys, label="é€‰æ‹©è¦åˆ é™¤çš„çŸ¥è¯†åºå·\U0001F600"),
                                 outputs=gr.Textbox(label="å·²åˆ é™¤çš„èƒ¡æ¡ƒçŸ¥è¯†ğŸ“š"),
                                 allow_flagging="never")


        with gr.Tab("æ³¨å…¥ä¸€äº›çŸ¥è¯†\U0001F4D6"):
            def split_text_and_add_to_knowledge(content):
                # è‹¥æºå†…å®¹è¿‡é•¿å°±å…ˆåˆ‡åˆ†
                tokens = len(content)
                max_tokens = 500
                splited = False

                # å¦‚æœ tokens æ•°é‡è¶…è¿‡äº†é™åˆ¶ï¼Œè¿›è¡Œåˆ‡åˆ†å¤„ç†
                if tokens > max_tokens:
                    splited = True
                    segments = Perception.split_text(content, min_length=max_tokens, buffer_min_length=int(max_tokens*0.3))
                    pairs = Perception.get_text_embedding_pairs(segments)
                    hutao.brain.add_knowledge_list(pairs)
                    save_agent_json(hutao.brain)

                    pairs_str = ""
                    for idx, pair in enumerate(pairs, 0):
                        pairs_str += (f"çŸ¥è¯†å•å…ƒ{idx}\n"
                                      f"çŸ¥è¯†æè¿°:\n{pair['text']}\n"
                                      f"åµŒå…¥å‘é‡å¤§å°:{len(pair['embedding'])}\n"
                                      f"{'-' * 40}\n")

                    return pairs_str, splited

                else:
                    knowledge = hutao.brain.add_knowledge_from_text(content)
                    save_agent_json(hutao.brain)
                    knowledge_str = (f"çŸ¥è¯†æè¿°:\n{knowledge['text']}\n"
                                     f"åµŒå…¥å‘é‡å¤§å°:{len(knowledge['embedding'])}\n"
                                     f"{'-' * 40}\n")

                    return knowledge_str, splited

            def add_knowledge_from_pdf(pdf_path):
                # å¯¹pdfè¿›è¡Œåˆ‡åˆ†ï¼Œç›´æ¥åŠ è½½åˆ°çŸ¥è¯†åº“
                pdf_content = hutao.perception.read_pdf(pdf_path)
                pairs_str, splited = split_text_and_add_to_knowledge(pdf_content)

                if splited:
                    pdf_str = (f"åŸºäºPDF:{pdf_path}åŠ è½½äº†å¦‚ä¸‹å†…å®¹:\n\n{pdf_content}"
                               f"ç”±äºçŸ¥è¯†æºæ–‡æœ¬è¿‡é•¿è€Œè¿›è¡Œäº†åˆ‡åˆ†ï¼š\n\n{pairs_str}")
                else:
                    pdf_str = (f"åŸºäºPDF:{pdf_path}åŠ è½½äº†å¦‚ä¸‹å†…å®¹:\n\n{pdf_content}"
                               f"\n\n{pairs_str}")
                return pdf_str

            gr.Interface(fn=add_knowledge_from_pdf,
                         title="å°†PDFæ–‡ä»¶çš„çŸ¥è¯†æ³¨å…¥åˆ°çŸ¥è¯†åº“\U0001F600",
                         inputs=gr.File(label="ä¸Šä¼ PDFğŸ“"),
                         outputs=gr.Textbox(label="ä»PDFæå–åˆ°çš„çŸ¥è¯†ğŸ“"),
                         allow_flagging="never")

            def add_knowledge_from_wiki(search_query):
                # è·å–wikiå†…å®¹
                wiki_object = hutao.action.use_wiki(search_query)
                wiki_url = wiki_object['url']
                wiki_content = wiki_object['content']

                pairs_str, splited = split_text_and_add_to_knowledge(wiki_content)
                if splited:
                    wiki_str = (f"åŸºäºWikiçš„æŸ¥è¯¢:{search_query}\næ‰¾åˆ°äº†url:{wiki_url},å†…å®¹å¦‚ä¸‹ï¼š"
                                f"\n\n{wiki_content}\n\nç”±äºçŸ¥è¯†æºæ–‡æœ¬è¿‡é•¿è€Œè¿›è¡Œäº†åˆ‡åˆ†ï¼š\n\n{pairs_str}")
                else:
                    wiki_str = (f"åŸºäºWikiçš„æŸ¥è¯¢:{search_query}\næ‰¾åˆ°äº†url:{wiki_url},å†…å®¹å¦‚ä¸‹ï¼š"
                                f"\n\n{wiki_content}\n\n{pairs_str}")
                return wiki_str

            gr.Interface(fn=add_knowledge_from_wiki,
                         title="ä»Wikiæ£€ç´¢çŸ¥è¯†å¹¶æ³¨å…¥åˆ°çŸ¥è¯†åº“\U0001F600",
                         inputs=gr.Textbox(label="æœç´¢Wikiç™¾ç§‘ğŸ“"),
                         outputs=gr.Textbox(label="ä»Wikiæå–åˆ°çš„çŸ¥è¯†ğŸ“"),
                         allow_flagging="never")

        with gr.Tab("å¸¸ç”¨çš„Prompts\U0001F4AD"):
            # åŠ è½½é¢„åˆ¶prompt
            def display_prompt(key: str) -> str:
                prompts = load_prompts("../resource/key_prompt.json")
                prompt = prompts.get(key, 'æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„prompt')
                return prompt

            prompt_keys = load_prompts('../resource/key_prompt.json').keys()  # è·å–æ‰€æœ‰çš„key
            iface = gr.Interface(display_prompt,
                                 gr.Dropdown(prompt_keys, label="éœ€è¦çš„PromptåŠŸèƒ½ğŸŒ±"),
                                 gr.Textbox(label="é€šç”¨Promptå†…å®¹ğŸ’¬", show_copy_button = True),
                                 allow_flagging="never")

    demo.queue().launch(share=False)
